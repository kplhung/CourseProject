<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE nitf SYSTEM "http://www.nitf.org/IPTC/NITF/3.3/specification/dtd/nitf-3-3.dtd">
<nitf change.date="June 10, 2005" change.time="19:30" version="-//IPTC//DTD NITF 3.3//EN">
  <head>
    <title>An Ever-Changing Course: Taking Admissions Tests On Computer</title>
    <meta content="TESTT" name="slug"/>
    <meta content="6" name="publication_day_of_month"/>
    <meta content="8" name="publication_month"/>
    <meta content="2000" name="publication_year"/>
    <meta content="Sunday" name="publication_day_of_week"/>
    <meta content="Education Life Supplement" name="dsk"/>
    <meta content="32" name="print_page_number"/>
    <meta content="4A" name="print_section"/>
    <meta content="1" name="print_column"/>
    <meta content="Technology; Education; U.S." name="online_sections"/>
    <docdata>
      <doc-id id-string="1220302"/>
      <doc.copyright holder="The New York Times" year="2000"/>
      <identified-content>
        <classifier class="indexing_service" type="descriptor">Colleges and Universities</classifier>
        <classifier class="indexing_service" type="descriptor">Tests and Testing</classifier>
        <classifier class="indexing_service" type="descriptor">Graduate Record Examination</classifier>
        <classifier class="indexing_service" type="descriptor">Education and Schools</classifier>
        <classifier class="indexing_service" type="descriptor">Computers and the Internet</classifier>
        <person class="indexing_service">Guernsey, Lisa</person>
        <classifier class="online_producer" type="taxonomic_classifier">Top/News/U.S.</classifier>
        <classifier class="online_producer" type="taxonomic_classifier">Top/News/Education</classifier>
        <classifier class="online_producer" type="taxonomic_classifier">Top/News/Technology</classifier>
        <classifier class="online_producer" type="taxonomic_classifier">Top/Classifieds/Job Market/Job Categories/Education</classifier>
        <classifier class="online_producer" type="taxonomic_classifier">Top/Classifieds/Job Market/Job Categories/Technology, Telecommunications and Internet</classifier>
        <classifier class="online_producer" type="general_descriptor">Colleges and Universities</classifier>
        <classifier class="online_producer" type="general_descriptor">Computers and the Internet</classifier>
        <classifier class="online_producer" type="general_descriptor">Tests and Testing</classifier>
        <classifier class="online_producer" type="general_descriptor">Education and Schools</classifier>
      </identified-content>
    </docdata>
    <pubdata date.publication="20000806T000000" ex-ref="http://query.nytimes.com/gst/fullpage.html?res=9A03E6DD113DF935A3575BC0A9669C8B63" item-length="2173" name="The New York Times" unit-of-measure="word"/>
  </head>
  <body>
    <body.head>
      <hedline>
        <hl1>An Ever-Changing Course: Taking Admissions Tests On Computer</hl1>
      </hedline>
      <byline class="print_byline">By LISA GUERNSEY</byline>
      <byline class="normalized_byline">Guernsey, Lisa</byline>
      <abstract>
        <p>Article on taking standard admissions tests, such as Graduate Record Examination, on computer; explains complexity of computerized adaptive tests and how they differ from paper exams, with advice to test-takers; sample questions (special section, Education Life) (M)</p>
      </abstract>
    </body.head>
    <body.content>
      <block class="lead_paragraph">
        <p>WITH nearly everything becoming electronic and digital, it may be no surprise to discover that the hum of hard drives has replaced the scent of freshly sharpened pencils for students taking many of today's standardized admissions tests.</p>
        <p>But here is the catch: most computerized tests are not merely onscreen versions of what appears in paper workbooks. Rather than simply scanning in copies of multiple choice examinations, some test makers have prepared adaptive tests, meaning that they are designed to adapt, on the fly, to the ability of the person taking the test.</p>
      </block>
      <block class="full_text">
        <p>WITH nearly everything becoming electronic and digital, it may be no surprise to discover that the hum of hard drives has replaced the scent of freshly sharpened pencils for students taking many of today's standardized admissions tests.</p>
        <p>But here is the catch: most computerized tests are not merely onscreen versions of what appears in paper workbooks. Rather than simply scanning in copies of multiple choice examinations, some test makers have prepared adaptive tests, meaning that they are designed to adapt, on the fly, to the ability of the person taking the test.</p>
        <p>For example, those who take the Graduate Record Examination, which is often required for admission to graduate schools, receive one test question or item at a time. If they answer the first question correctly, the computer serves them another question of greater difficulty. If the answer to the first question is wrong, the next question is easier. And so it goes throughout the exam. The items on the screen at any time depend entirely on how the previous question was answered. People who answer some right and some wrong are led down a zigzag path: A wrong answer bumps them down to an easier level, but a correct answer lifts them back to more difficult ones.</p>
        <p>It is like zeroing in on a target, designers of the tests say. Each answered question provides another tidbit of information about the test taker's ability, enabling the computer to shift course until it comes upon those questions that lie just between the ones that are answered easily and the ones that stump.</p>
        <p>''Toward the end of the test, you come to items that are most difficult for you,'' said Howard Wainer, the author of ''Computerized Adaptive Testing: A Primer.'' The second edition of the book was published in the spring. ''Those are the questions that are at your ability.''</p>
        <p>The move to offer standardized tests on computers has brought both criticism and kudos. Many students, being accustomed to doing most of their schoolwork on computers, shrink at the notion of completing anything on paper. What's more, many of the computerized tests provide scores immediately after the last item is answered, which the students like.</p>
        <p>Critics of standardized tests, who contend that both paper and computerized tests are flawed in determining intellectual ability, also argue that computerized exams put people unaccustomed to the technology at a disadvantage. Besides that, the fee to take the digital tests is twice as much as for their paper predecessors.</p>
        <p>The adaptive format also raises questions about how test scores can be fairly compared. Because the test items change as the test is taken, two people taking what is officially the same test at the same time are likely to answer completely different questions.</p>
        <p>Adding to the confusion, raw totals mean nothing. Two test takers could get the same number of questions correct and yet come up with widely varying scores. As Dr. Wainer explains, adaptive exams are designed so that, in theory, everyone gets about half the questions right. Highly proficient test takers will be given harder questions and may get only half of them right, and the less proficient will be given easier questions and may also correctly answer 50 percent.</p>
        <p>The scores are derived by assigning different values to different items, with greater weight given to difficult items than easy ones. The weighting is generally not described to test takers, because it involves complicated mathematics. So for people who have grown up with traditional test scores based on raw totals (and who will forever equate 50 percent with a failing grade), the concept takes some getting used to.</p>
        <p>''It is hard to explain how the scoring works,'' said Stephen T. Schreiber, vice president of the Law School Admission Council, ''and that is one of the reasons why testing services have to be careful when they go to computerized testing.''</p>
        <p>TEST takers' confusion over both the scoring and the zigzag pattern of adaptive testing has led many of them to adopt new strategies, some of which test developers consider inadvisable.</p>
        <p>Consider the case of Ricken Patel, a graduate student in Harvard University's public-policy program, who took the computerized Graduate Record Examination last year. ''I was advised to concentrate on the first few questions,'' he said. As he understood it, if the first item were answered incorrectly, he would be led down a path of easy items instead of hard ones, and would get less credit for correct answers to easy ones.</p>
        <p>When he took the test, he ran out of time and guessed on the last few answers. Still, he scored higher on the test than he had expected, so he concluded that his strategy had worked.</p>
        <p>But officials at Educational Testing Service, the country's largest developer of standardized tests, say that emphasizing the first few questions is not, in fact, a smart way to approach the tests.</p>
        <p>Dr. Wainer, a principal research scientist at Educational Testing, groans when he hears of students who worry mostly about the first few questions. ''To suggest that you should pay close attention to the first 10 is correct,'' he said. ''But you should pay close attention to the next 10, too.''</p>
        <p>A test taker should concentrate on each question throughout the time period, according to Drew Gitomer, vice president of research at Educational Testing Service. Suppose that a wrongly answered item at the end causes an easier item to appear. If a test taker were not concentrating by that point, thinking that the last few items mattered little, she might answer one wrongly, then receive an easier question and answer that wrongly too.</p>
        <p>''Pacing yourself all along is very important,'' Dr. Gitomer said.</p>
        <p>Yet the misperception remains. Many students, when interviewed, said they worried that if they messed up on the first few questions, they would never be able to redeem themselves.</p>
        <p>Michele Rogers, assistant dean of admissions at Northwestern University's Kellogg Graduate School of Management, said she thought that test takers made that mistake because of another factor: most of the exams do not allow students to return to questions they have answered. ''People confuse the two concepts,'' she said, ''and that compounds the problem.''</p>
        <p>The testing service expects that some of the myths surrounding the tests will dissipate as the tests become more mainstream. The Graduate Record Examination, which was first administered via computer in 1992, is now almost entirely computerized. So are admission tests for students of nursing and dental schools, as well as the Test of English as a Foreign Language. And the Graduate Management Admission Test, which is used by business schools, has been offered only on computer since 1997.</p>
        <p>Other tests are still coming: The SAT is many years away from being offered on computer, testing service officials say. The agency first wants to make sure that all high school students are at ease with computers, and it is waiting for a day in which it is logistically possible to administer not just thousands but millions of exams in computer centers throughout the world. The A.C.T. Assessment, which is a competitor of the SAT, is not yet offered on computer for the same reasons.</p>
        <p>A few professional tests, notably the Law School Admission Test and the Medical College Admission Test, are still administered on paper, but officials for the tests expect that computerized versions will replace them in the next decade.</p>
        <p>As computerized tests become the norm, the testing companies must face the challenge of being sure that the results will be as useful to admissions officers in evaluating applicants as paper tests were in the past. Yet is it fair to compare a test taken on paper, in a linear fashion, with one that uses the adaptive method?</p>
        <p>The question makes test developers squirm. They stress that they have made sure that the same types of questions are asked in both cases, and contend that adaptive tests are more efficient than traditional paper tests in determining a person's ability. They have conducted studies, they say, to show that comparisons can be made fairly, given the right items and the right scoring systems.</p>
        <p>Dr. Gitomer, for example, calls the computerized tests ''more precise.'' Even so, he added, ''It's not perfect -- measurement of human abilities is not an exact science.'' That is why, he continues, ''we advise people never to use the test score as the only instrument'' in evaluating applicants.</p>
        <p>EVEN when comparisons are made between two tests in the adaptive format, another question comes up: How can a person taking a test with items X, Y and Z be compared to a person taking a test with items A, B and C?</p>
        <p>In this case, test developers have history on their side: Such comparisons have long been part of standardized testing. To avert cheating on paper-and-pencil exams, administrators often give students versions that differ slightly from the exams their peers took a month or a year earlier.</p>
        <p>To ensure that scores are fair, test administrators employ a statistical methodology called equating. The idea is that different items may act as similar measures of ability. Two different algebra equations, for example, may measure a certain level of algebraic ability. Regardless of whether the questions appear on paper or on screen, test developers say, items can be equated to derive solid scores.</p>
        <p>Indeed, many admissions officers say that they have found both paper and computerized tests to be equally helpful in indicating a student's ability, if used along with other information like grade point averages.</p>
        <p>Ms. Rogers of Kellogg said, ''We often didn't know the difference,'' in scores from paper and adaptive tests when both were used a few years ago. Now many business schools do not even accept G.M.A.T. scores from paper tests, because they are too old.</p>
        <p>But business schools have noticed at least one difference: G.M.A.T. scores have been rising in recent years. No studies have been done to determine why, but admissions officers have theories. Maybe students are doing better because they are more accustomed to computers than paper. Maybe they perform better because the test is now administered on a more flexible schedule. Maybe, because of the roaring economy, business schools are attracting smarter applicants than they once did.</p>
        <p>Whatever the reason, and regardless of whether scores are climbing, graduate schools like the computerized tests for another reason: because applicants can take the test at almost any time, their scores arrive at admissions offices in a constant flow, instead of in huge batches once or twice a year.</p>
        <p>Admissions officers now have the luxury of reviewing scores on their own schedule. ''We can make decisions at any time throughout the season,'' Ms. Rogers said.</p>
        <p>TIPS FOR STUDENTS</p>
        <p>What should test takers worry about when taking computerized adaptive tests? A few elements of strategy are worth heeding, developers and test-preparation coaches say. For example, as Drew Gitomer, vice president of research at the Educational Testing Service, said, pace yourself.</p>
        <p>Do not attempt to skip questions. Every question must be answered. Most computerized tests will not display a new item until the previous one is answered. This also means that a person cannot browse through the questions before answering them. ''You have to just jump in,'' said Trent Anderson, vice president and publisher of Kaplan Inc., a test preparation and educational services company.</p>
        <p>Emily Buenting, a recent graduate of Simmons College's business school, said she liked that aspect of the computerized G.M.A.T. when she took it in 1998. ''I actually found it much better to take the test on the computer,'' said Ms. Buenting, who had previously taken it on paper. ''It forced me to be more organized, rather than always skipping around.''</p>
        <p>Another difference is the flexible schedule that usually comes with the administration of computer-ized adaptive tests. Instead of offering the tests only twice a year in huge auditoriums on Saturday mornings, E.T.S. and other test administrators allow students to schedule their own tests . They arrange to meet exam proctors at local computer laboratories, typically run by a Thomson Learning company called Prometric.</p>
        <p>The flexibility sounds great, but it can throw test takers for a loop, Mr. Anderson of Kaplan said. He said he had seen more students starting the test prep-aration process later since the advent of computerized tests. ''The burden is on the individual to initiate the process,'' he said. ''And that means you have to plan ahead.''</p>
        <p>LISA GUERNSEY</p>
        <p>Lisa Guernsey writes about technology for The New York Times.</p>
      </block>
    </body.content>
    <body.end>
      <tagline class="author_info">Lisa Guernsey writes about technology for The New York Times.</tagline>
    </body.end>
  </body>
</nitf>
